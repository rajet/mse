{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ost_logo.png\" width=\"240\" height=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Machine Learning </b> <br> MSE FTP MachLe <br> \n",
    "<a href=\"mailto:christoph.wuersch@ost.ch\"> Christoph Würsch </a> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Curse of Dimensionality\n",
    "\n",
    "\n",
    "\n",
    "- MSE FTP_MachLE\n",
    "- *Author: Prof. Dr. Christoph Würsch, Institute for Computational Engineering ICE, OST*\n",
    "- based on a blog by Martin Thoma: [link](https://martin-thoma.com/curse-of-dimensionality/)\n",
    "\n",
    "In machine learning, the \"curse of dimensionality\" is often stated but much less often explained. At least not in detail. One just gets told that points are farer away from each other in high dimensional spaces.\n",
    "\n",
    "## 1. Maximum minimal distance ¶\n",
    "One approach to this is to calculate the maximum minimal distance of \n",
    "\n",
    "One approach to this is to calculate the maximum minimal distance of $k$ points in $[0,1]$. So you try to place \n",
    "$k$ points in such a way, that the minimum over the pairwise distances of those $k$ points is maximal. Let's call this $\\alpha(n,k)$. However, it is not easily possible to calculate $\\alpha(n,k)$ for arbitrary $n>2$.\n",
    "\n",
    "But the special case $k=2$ and $n=2$ it is easy:\n",
    "\n",
    "- $\\alpha(n,2)=\\sqrt{n}$\n",
    "- $\\alpha(n,2^n)=1$\n",
    "\n",
    "So you can see that two points get can be farer apart in higher dimensions and that it needs much more points in higher dimensions to force at least two of them to have distance $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Average distance\n",
    "\n",
    "Another approach is to calculate the average Euclidian distance of $k$ uniformly randomly sampled points in $[0,1]^n$. Let's call it \n",
    "$\\beta(n,k)$.\n",
    "\n",
    "One first insight is that $\\beta(n,k)=\\beta(n,j)$ for $k,j\\ge2$. Hence, we will only use $\\beta(n)$ in the following.\n",
    "It is possible to calculate this, but it is rather tedious ([see link](https://math.stackexchange.com/questions/1254129/average-distance-between-two-random-points-in-a-square/1254154#1254154)).\n",
    "\n",
    "Just two calculated solutions for $k=2$:\n",
    "\n",
    "- $\\beta(1) = \\frac{1}{3}$\n",
    "- $\\beta(2) = \\frac{2+\\sqrt{2}+5\\operatorname{arcsinh}(1)}{15}=\\frac{2+\\sqrt{2}+5\\log(1+\\sqrt{2})}{15} \\approx 0.52140543316472\\ldots$\n",
    "\n",
    "We have to compute:\n",
    "$$I=\\int_{[0,1]^4}\\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}\\,d\\mu. \\tag{1}$$\n",
    "Assuming that $X_1$ and $X_2$ are two independent random variables, uniformly distributed over [0,1], the pdf of their difference $\\Delta X=X_1−X_2$ is given by:\n",
    "\n",
    "$$I=\\int_{[0,1]^4}\\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}\\,d\\mu. \\tag{1}$$\n",
    "\n",
    "hence:\n",
    "$$\\begin{eqnarray*} I &=& \\iint_{[-1,1]^2}(1-|x|)(1-|y|)\\sqrt{x^2+y^2}\\,dx\\,dy \\\\&=&4\\iint_{[0,1]^2}xy\\sqrt{(1-x)^2+(1-y)^2}\\,dx\\,dy\\tag{3}\\end{eqnarray*}$$\n",
    "that is tedious to compute but still possible; we have:\n",
    "\n",
    "$$I = \\frac{2+\\sqrt{2}+5\\operatorname{arcsinh}(1)}{15}=\\frac{2+\\sqrt{2}+5\\log(1+\\sqrt{2})}{15}=0.52140543316472\\ldots$$\n",
    "\n",
    "In general, this distance is very hard to compute, see [MathWorld](https://mathworld.wolfram.com/HypercubeLinePicking.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A simple upper bound for the average Euclidian distance\n",
    "\n",
    "A simple argument via **Jensen's inequality** gives an upper bound; we first note that if $X=(X_1,…,X_n)$ is uniform on $[0,1]^n$ then each of the marginals $X_i \\in [0,1]$ are independent and uniform. Then\n",
    "\n",
    "\\begin{aligned}\n",
    "\\beta(n)= \\mathbb{E} \\left[ |X - Y|\\right] \n",
    "& = \\mathbb{E} \\left[ \\left( \\sum_{i=1}^n(X_i - Y_i)^2 \\right)^{1/2} \\right] \\\\ \n",
    "& \\leq \\mathbb{E} \\left[ \\sum_{i=1}^n(X_i - Y_i)^2 \\right]^{1/2} \n",
    " = \\sqrt{n} \\cdot \\mathbb{E} [(X_1 - Y_1)^2]^{1/2}\n",
    " = \\sqrt{\\frac{n}6}\n",
    "\\end{aligned}\n",
    "\n",
    "This is because the expectation value $\\mathbb{E}$ of the distance squared for $n=1$ is given by:\n",
    "\\begin{aligned}\n",
    "\\mathbb{E} [\\Delta x^2] &= \\int \\int \\left[ p(x)\\cdot x - p(x')\\cdot x' \\right]^2 dx dx' \\\\\n",
    "& =\\int_0^1 \\int_0^1 (x - x')^2 dx dx'\\\\\n",
    "& =\\int_0^1 \\int_0^1 (x^2-2xx' + x'^2) dx dx' =\\frac{1}{6}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed the pseudorandom number generator\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "from numpy.random import rand\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N=100\n",
    "seed(1)\n",
    "beta=list()\n",
    "DimList=np.linspace(2,200)\n",
    "\n",
    "\n",
    "for d in DimList:\n",
    "    # seed random number generator\n",
    "    # generate some random numbers\n",
    "    x=rand(np.int32(d),N)\n",
    "    y=rand(np.int32(d),N)\n",
    "    #print(np.shape(x))\n",
    "    z=np.power(x-y,2)\n",
    "    #print(np.shape(z))\n",
    "    beta.append(np.mean(np.sqrt(np.sum(z,axis=0))))\n",
    "\n",
    "\n",
    "plt.figure\n",
    "plt.plot(beta)\n",
    "plt.grid()\n",
    "plt.xlabel('dimension')\n",
    "plt.ylabel('average distance')\n",
    "# reset the seed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Density of Hypercubes\n",
    "\n",
    "One interesting question is how much of the $n$-dimensional hypercube can be filled by one inscribed $n$-dimensional hyperball. The volume of an $n$-dimensional hypercube is $V_C(a) = a^n$ where $a$ is the cubes side length. So for one dimension it is $a$, for 2 dimensions (a square) it is $a^2$ and for $n=3$ it is the volume of a cube.\n",
    "\n",
    "\n",
    "The volume of an $n$-dimensional ball is given by:\n",
    "\n",
    "$$V_S(r) = r^n \\cdot \\frac{\\pi^{n/2}}{\\Gamma (\\frac{n}{2} + 1)}$$\n",
    "\n",
    "\\begin{aligned}\n",
    "n=1: \\quad & r \\cdot \\frac{\\sqrt{\\pi}}{\\Gamma(1.5)} = r \\frac{\\sqrt{\\pi}}{0.5 \\Gamma(0.5)} = 2r \\\\\n",
    "n=2: \\quad & r^2 \\cdot \\frac{\\pi}{\\Gamma (2)} = r^2 \\frac{\\pi}{\\Gamma (1)} = r^2 \\pi \\\\\n",
    "n=3: \\quad & r^3 \\cdot \\frac{\\pi^{3/2}}{\\Gamma (\\frac{5}{2})} = r^3 \\frac{\\pi^{3/2}}{1.5 \\cdot 0.5 \\cdot \\Gamma (\\frac{1}{2})} = r^3 \\frac{\\pi}{\\frac{3}{4}}\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "This means the __percentage of space of a unit hypercube which can be filled by the biggest inscribed hyperball__ is:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{V_S(0.5)}{V_C(1)}\n",
    "&= \\frac{r^n \\frac{\\pi^{n/2}}{\\Gamma (\\frac{n}{2} + 1)}}{1} \n",
    "= \\frac{0.5^n \\pi^{n/2}}{\\Gamma (\\frac{n}{2} + 1)} \\\\\n",
    "&= \\frac{0.5^n \\pi^{n/2}}{\\frac{n}{2} \\cdot \\Gamma (\\frac{n}{2})} \n",
    "= \\frac{0.5^n \\cdot 2 \\cdot \\pi^{n/2}}{n \\cdot \\frac{2 \\frac{n}{2}!}{n}} \\\\\n",
    "&= \\frac{0.5^n \\cdot \\pi^{n/2}}{\\frac{n}{2}!}\n",
    "\\end{align}\n",
    "\n",
    "__You can see that this term goes to zero with increasing dimension. This means most of the volume is not in the center, but in the edges of the $n$ dimensional hypercube. It also means that $k$ nearest neighbors with Euclidean Distance measure will need enormously large spheres to get to the next neighbours__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Average Angle\n",
    "\n",
    "One interesting question is how the average angle between two points (and the origin) changes with higher dimensions. Suppose all points are in the $[0,1]^n$ hypercube.\n",
    "\n",
    "I thought about this for a while and came to the conclusion that it should be 90° in average due to symmetry. No matter how high the dimension is, because the higer the dimension $n$ becomes, the closer the data is concentrated to the axes.\n",
    "\n",
    "A short experiment confirms that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cosine_dist(p1, p2):\n",
    "    \"\"\"\n",
    "    Calculate the cosine distance between to points in R^n.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> cosine_dist([1, 0], [0, 1])\n",
    "    90.0\n",
    "    >>> cosine_dist([1, 0], [2, 0])\n",
    "    0.0\n",
    "    >>> cosine_dist([1, 0], [-1, 0])\n",
    "    180.0\n",
    "    \"\"\"\n",
    "    ang = 1 - spatial.distance.cosine(p1, p2)\n",
    "    if not (-1 <= ang <= 1):\n",
    "        if ang >= 1:\n",
    "            return 0\n",
    "        if ang <= -1:\n",
    "            return 180\n",
    "    return np.degrees(np.arccos(ang))\n",
    "\n",
    "\n",
    "def get_angles(n, num_points=100):\n",
    "    \"\"\"Get angles of random points in n-dimensional unit hypercube.\"\"\"\n",
    "    points = 2 * np.random.rand(num_points, n) - 1\n",
    "    angles = []\n",
    "    for p1 in points:\n",
    "        for p2 in points:\n",
    "            angles.append(cosine_dist(p1, p2))\n",
    "    return angles\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "\n",
    "    doctest.testmod()\n",
    "    for n in [2, 3, 4, 5, 6, 7, 8, 9, 10, 100, 1000, 10000]:\n",
    "        angles = get_angles(n)\n",
    "        print(\"{:>5} dim: {:0.4f} avg angle\".format(n, sum(angles) / len(angles)))\n",
    "        sns.displot(angles, kde=False, rug=False)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
